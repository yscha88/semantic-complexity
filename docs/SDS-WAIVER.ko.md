# SDS: Semantic Complexity Measurement & Waiver System

## 1. ê°œìš”

### 1.1 ëª©ì 
SRSì—ì„œ ì •ì˜ëœ ìˆ˜í•™ì  í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•œ ì„¤ê³„ ëª…ì„¸.

### 1.2 ë²”ìœ„
- ë³µì¡ë„ ì¸¡ì • (5D ë²¡í„°, ì •ì¤€ í¸ì°¨)
- ì—ë„ˆì§€ ê³„ì‚° ë° ìˆ˜ë ´ íŒì •
- ê²½ê³„ íë¦„ ë¶„ì„
- ë¶„ì„ View (Hotspot, Flux, ROI)
- ADR ê¸°ë°˜ Waiver ê´€ë¦¬

---

## 2. ì•„í‚¤í…ì²˜

### 2.1 ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°

```
semantic_complexity/
â”œâ”€â”€ measurement/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector.py         # 5D ë²¡í„° ì¸¡ì •
â”‚   â”œâ”€â”€ deviation.py      # ì •ì¤€ í¸ì°¨ ê³„ì‚°
â”‚   â”œâ”€â”€ hodge.py          # Hodge bucket ë¶„ë¥˜
â”‚   â””â”€â”€ evidence.py       # rule_hits ìˆ˜ì§‘
â”œâ”€â”€ energy/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ potential.py      # Î¦(k) ê³„ì‚°
â”‚   â”œâ”€â”€ delta.py          # Î”Î¦ ê³„ì‚°
â”‚   â””â”€â”€ convergence.py    # Îµ-ìˆ˜ë ´ íŒì •
â”œâ”€â”€ flux/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ boundary.py       # ê²½ê³„ íë¦„ ê³„ì‚°
â”‚   â””â”€â”€ degradation.py    # ê²½ê³„ ì•…í™” íƒì§€
â”œâ”€â”€ view/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hotspot.py        # View A: Hotspot Trajectory
â”‚   â”œâ”€â”€ boundary.py       # View B: Boundary Flux
â”‚   â””â”€â”€ roi.py            # View C: Refactor ROI
â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ entity.py         # Entity ê´€ë¦¬
â”‚   â”œâ”€â”€ snapshot.py       # Snapshot ê´€ë¦¬
â”‚   â”œâ”€â”€ edge.py           # Edge ê´€ë¦¬
â”‚   â””â”€â”€ store.py          # ì €ì¥ì†Œ (SQLite/JSON)
â”œâ”€â”€ gate/
â”‚   â”œâ”€â”€ waiver.py         # Waiver í†µí•© ì²´í¬
â”‚   â”œâ”€â”€ mvp.py            # Gate ë¡œì§
â”‚   â””â”€â”€ adr/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ schema.py     # ADR ìŠ¤í‚¤ë§ˆ
â”‚       â”œâ”€â”€ parser.py     # ADR íŒŒì„œ
â”‚       â”œâ”€â”€ validator.py  # ìœ íš¨ì„± ê²€ì¦ (ìˆ˜ë ´ í¬í•¨)
â”‚       â””â”€â”€ expiry.py     # ë§Œë£Œ ê´€ë¦¬
â””â”€â”€ mcp/
    â””â”€â”€ __init__.py       # MCP í†µí•©
```

### 2.2 ë°ì´í„° íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MEASUREMENT PHASE                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Source Code â”€â”€â–¶ AST Parser â”€â”€â–¶ 5D Vector x_u                   â”‚
â”‚                      â”‚              â”‚                            â”‚
â”‚                      â–¼              â–¼                            â”‚
â”‚               rule_hits[]    Hodge Bucket                        â”‚
â”‚               (Evidence)                                         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEVIATION PHASE                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  x_u + architectureRole â”€â”€â–¶ Î¼_t(u) â”€â”€â–¶ d_u = â€–x_u/Î¼_t - 1â€–â‚‚          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENERGY PHASE                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Î£d_u + Î£w(e) + OpsPenalty â”€â”€â–¶ Î¦(k)                             â”‚
â”‚                                    â”‚                             â”‚
â”‚  Î¦(k) - Î¦(k-1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Î”Î¦                           â”‚
â”‚                                    â”‚                             â”‚
â”‚  |Î”Î¦| < Îµ? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Converged?                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ADR ELIGIBILITY                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Converged âˆ§ Flux stable âˆ§ Evidence complete âˆ§ Gate failed      â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚                    ADR ë°œê¸‰ ê°€ëŠ¥                                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. ë°ì´í„° ëª¨ë¸

### 3.1 Entity

```python
from dataclasses import dataclass, field
from enum import Enum
from typing import Literal


class EntityType(Enum):
    MODULE = "module"
    FILE = "file"
    FUNC = "func"
    OBJECT = "object"


@dataclass
class Entity:
    """ì½”ë“œ ì—”í‹°í‹° (ì•ˆì •ì  ì •ì²´ì„±)"""
    entity_id: str                    # stable identifier (hash)
    type: EntityType
    path: str                         # file path
    symbol: str                       # function/class name
    language: Literal["python", "typescript", "go"]

    @property
    def qualified_name(self) -> str:
        return f"{self.path}:{self.symbol}"
```

### 3.2 Snapshot

```python
from datetime import datetime


@dataclass
class Snapshot:
    """ì»¤ë°‹ ë‹¨ìœ„ ìŠ¤ëƒ…ìƒ·"""
    snapshot_id: str                  # auto-generated
    commit: str                       # git commit hash
    timestamp: datetime
    repo: str                         # repository name
    service: str | None = None        # service name (monorepo)
    env: Literal["dev", "prod"] = "dev"
```

### 3.3 Metrics

```python
import numpy as np
from numpy.typing import NDArray


@dataclass
class ComplexityVector:
    """5D ë³µì¡ë„ ë²¡í„°"""
    C: float    # Control
    N: float    # Nesting
    S: float    # State
    A: float    # Async
    L: float    # Coupling (Î›)

    def to_array(self) -> NDArray[np.float64]:
        return np.array([self.C, self.N, self.S, self.A, self.L])

    @classmethod
    def from_array(cls, arr: NDArray[np.float64]) -> "ComplexityVector":
        return cls(C=arr[0], N=arr[1], S=arr[2], A=arr[3], L=arr[4])


class HodgeBucket(Enum):
    ALGORITHMIC = "algorithmic"       # C + N
    BALANCED = "balanced"             # A
    ARCHITECTURAL = "architectural"   # S + Î›


@dataclass
class Metrics:
    """ì—”í‹°í‹°ë³„ ë©”íŠ¸ë¦­"""
    entity_id: str
    snapshot_id: str

    # 5D ë²¡í„°
    x: ComplexityVector

    # íŒŒìƒ ê°’
    raw_sum: float                    # sum(x)
    tensor: NDArray[np.float64] | None = None  # optional tensor repr

    # ì •ì¤€ í¸ì°¨
    d: float = 0.0                    # deviation from canonical

    # ë¶„ë¥˜
    hodge: HodgeBucket = HodgeBucket.ALGORITHMIC
    architecture_role: str = "app"          # api/external, lib/domain, app

    # ì‹ ë¢°ë„
    confidence: float = 1.0           # 0.0 ~ 1.0
```

### 3.4 RuleHit (Evidence)

```python
@dataclass
class Location:
    """ì½”ë“œ ìœ„ì¹˜"""
    file: str
    line: int
    column: int | None = None
    ast_node_type: str | None = None  # e.g., "FunctionDef", "If"


@dataclass
class RuleHit:
    """ê·œì¹™ íˆíŠ¸ (ì¸¡ì • ê·¼ê±°)"""
    entity_id: str
    snapshot_id: str
    rule_id: str                      # e.g., "nesting/depth", "state/mutation"
    count: int
    locations: list[Location] = field(default_factory=list)

    def has_evidence(self) -> bool:
        return len(self.locations) > 0
```

### 3.5 Edge

```python
class EdgeType(Enum):
    IMPORT = "import"                 # module dependency
    CALL = "call"                     # function call
    INHERIT = "inherit"               # class inheritance
    BOUNDARY = "boundary"             # trust boundary crossing


@dataclass
class WeightComponents:
    """ê°„ì„  ê°€ì¤‘ì¹˜ êµ¬ì„±ìš”ì†Œ"""
    coupling: float = 0.0             # Î± coefficient
    boundary: float = 0.0             # Î² coefficient
    cognitive: float = 0.0            # Î³ coefficient
    failure_propagation: float = 0.0  # Î´ coefficient


@dataclass
class Edge:
    """ê·¸ë˜í”„ ê°„ì„ """
    src_entity: str                   # entity_id
    dst_entity: str                   # entity_id
    snapshot_id: str
    edge_type: EdgeType
    weight_components: WeightComponents

    @property
    def weight_total(self) -> float:
        w = self.weight_components
        return w.coupling + w.boundary + w.cognitive + w.failure_propagation

    def is_boundary_crossing(self) -> bool:
        return self.edge_type == EdgeType.BOUNDARY or self.weight_components.boundary > 0
```

---

## 4. ì¸¡ì • ëª¨ë“ˆ (measurement)

### 4.1 5D ë²¡í„° ì¸¡ì • (vector.py)

```python
import ast
from dataclasses import dataclass


@dataclass
class VectorMeasurement:
    """ë²¡í„° ì¸¡ì • ê²°ê³¼"""
    vector: ComplexityVector
    rule_hits: list[RuleHit]


class VectorAnalyzer:
    """5D ë³µì¡ë„ ë²¡í„° ë¶„ì„ê¸°"""

    def measure(self, source: str, entity_id: str, snapshot_id: str) -> VectorMeasurement:
        tree = ast.parse(source)

        rule_hits: list[RuleHit] = []

        # C: Control flow
        control = self._measure_control(tree, entity_id, snapshot_id, rule_hits)

        # N: Nesting depth
        nesting = self._measure_nesting(tree, entity_id, snapshot_id, rule_hits)

        # S: State complexity
        state = self._measure_state(tree, entity_id, snapshot_id, rule_hits)

        # A: Async complexity
        async_val = self._measure_async(tree, entity_id, snapshot_id, rule_hits)

        # Î›: Coupling
        coupling = self._measure_coupling(tree, entity_id, snapshot_id, rule_hits)

        vector = ComplexityVector(
            C=control,
            N=nesting,
            S=state,
            A=async_val,
            L=coupling,
        )

        return VectorMeasurement(vector=vector, rule_hits=rule_hits)

    def _measure_control(self, tree: ast.AST, entity_id: str,
                         snapshot_id: str, hits: list[RuleHit]) -> float:
        """ì œì–´ íë¦„ ë³µì¡ë„ ì¸¡ì •"""
        count = 0
        locations: list[Location] = []

        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.For, ast.While, ast.Try,
                                  ast.Match, ast.With)):
                count += 1
                locations.append(Location(
                    file="",  # filled by caller
                    line=node.lineno,
                    column=node.col_offset,
                    ast_node_type=type(node).__name__,
                ))

        if locations:
            hits.append(RuleHit(
                entity_id=entity_id,
                snapshot_id=snapshot_id,
                rule_id="control/branch",
                count=count,
                locations=locations,
            ))

        return float(count)

    def _measure_nesting(self, tree: ast.AST, entity_id: str,
                         snapshot_id: str, hits: list[RuleHit]) -> float:
        """ì¤‘ì²© ê¹Šì´ ì¸¡ì •"""
        max_depth = 0
        deepest_location: Location | None = None

        def walk_depth(node: ast.AST, depth: int = 0):
            nonlocal max_depth, deepest_location

            if isinstance(node, (ast.If, ast.For, ast.While, ast.Try,
                                  ast.With, ast.FunctionDef, ast.AsyncFunctionDef)):
                depth += 1
                if depth > max_depth:
                    max_depth = depth
                    deepest_location = Location(
                        file="",
                        line=node.lineno,
                        column=node.col_offset,
                        ast_node_type=type(node).__name__,
                    )

            for child in ast.iter_child_nodes(node):
                walk_depth(child, depth)

        walk_depth(tree)

        if deepest_location:
            hits.append(RuleHit(
                entity_id=entity_id,
                snapshot_id=snapshot_id,
                rule_id="nesting/depth",
                count=max_depth,
                locations=[deepest_location],
            ))

        return float(max_depth)

    # ... _measure_state, _measure_async, _measure_coupling ìœ ì‚¬ êµ¬í˜„
```

### 4.2 ì •ì¤€ í¸ì°¨ ê³„ì‚° (deviation.py)

```python
import numpy as np


# ëª¨ë“ˆ íƒ€ì…ë³„ ì •ì¤€ í”„ë¡œíŒŒì¼ Î¼_t
CANONICAL_PROFILES: dict[str, ComplexityVector] = {
    "api/external": ComplexityVector(C=3, N=2, S=1, A=1, L=2),
    "lib/domain": ComplexityVector(C=5, N=3, S=2, A=0, L=3),
    "app": ComplexityVector(C=8, N=4, S=3, A=2, L=5),
    "infra": ComplexityVector(C=4, N=2, S=2, A=3, L=4),
}


def get_canonical_profile(architecture_role: str) -> ComplexityVector:
    """ëª¨ë“ˆ íƒ€ì…ì˜ ì •ì¤€ í”„ë¡œíŒŒì¼ ë°˜í™˜"""
    return CANONICAL_PROFILES.get(architecture_role, CANONICAL_PROFILES["app"])


def calculate_deviation(x: ComplexityVector, architecture_role: str) -> float:
    """
    ì •ì¤€ í¸ì°¨ ê³„ì‚°

    d_u = â€–x_u / Î¼_t(u) - 1â€–â‚‚
    """
    mu = get_canonical_profile(architecture_role)

    x_arr = x.to_array()
    mu_arr = mu.to_array()

    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    mu_arr = np.where(mu_arr == 0, 1e-6, mu_arr)

    # ì •ê·œí™”ëœ í¸ì°¨
    normalized = x_arr / mu_arr - 1

    # L2 norm
    return float(np.linalg.norm(normalized))


def calculate_delta_deviation(d_before: float, d_after: float) -> float:
    """
    í¸ì°¨ ë³€í™”ëŸ‰ ê³„ì‚°

    Î”d = d(k) - d(k-1)

    Î”d < 0: ì •ì¤€ ìˆ˜ë ´ (ì¢‹ìŒ)
    Î”d > 0: ì •ì¤€ ì´íƒˆ (ë‚˜ì¨)
    """
    return d_after - d_before
```

### 4.3 Hodge Bucket ë¶„ë¥˜ (hodge.py)

```python
def classify_hodge(x: ComplexityVector) -> HodgeBucket:
    """
    Hodge bucket ë¶„ë¥˜

    algorithmic  = C + N     (ğŸ§€ Cheese)
    balanced     = A
    architectural = S + Î›    (ğŸ Bread + ğŸ¥“ Ham)
    """
    algorithmic = x.C + x.N
    balanced = x.A
    architectural = x.S + x.L

    max_val = max(algorithmic, balanced, architectural)

    if max_val == algorithmic:
        return HodgeBucket.ALGORITHMIC
    elif max_val == balanced:
        return HodgeBucket.BALANCED
    else:
        return HodgeBucket.ARCHITECTURAL


def get_hodge_scores(x: ComplexityVector) -> dict[str, float]:
    """Hodge bucketë³„ ì ìˆ˜"""
    return {
        "algorithmic": x.C + x.N,
        "balanced": x.A,
        "architectural": x.S + x.L,
    }
```

---

## 5. ì—ë„ˆì§€ ëª¨ë“ˆ (energy)

### 5.1 ì ì¬ í•¨ìˆ˜ ê³„ì‚° (potential.py)

```python
@dataclass
class PotentialConfig:
    """ì ì¬ í•¨ìˆ˜ ê°€ì¤‘ì¹˜ ì„¤ì •"""
    lambda_1: float = 1.0    # ì •ì¤€ í¸ì°¨ ê°€ì¤‘ì¹˜
    lambda_2: float = 0.5    # ê°„ì„  ê°€ì¤‘ì¹˜ (bad coupling)
    lambda_3: float = 0.3    # ìš´ì˜ í˜ë„í‹° ê°€ì¤‘ì¹˜


@dataclass
class PotentialResult:
    """ì ì¬ í•¨ìˆ˜ ê³„ì‚° ê²°ê³¼"""
    phi: float
    deviation_sum: float
    edge_weight_sum: float
    ops_penalty: float
    components: dict[str, float]


def calculate_phi(
    metrics: list[Metrics],
    edges: list[Edge],
    ops_penalty: float = 0.0,
    config: PotentialConfig | None = None,
) -> PotentialResult:
    """
    ì „ì—­ ì ì¬ í•¨ìˆ˜ ê³„ì‚°

    Î¦(k) = Î»â‚Â·Î£_u d_u(k) + Î»â‚‚Â·Î£_e w(e) + Î»â‚ƒÂ·OpsPenalty(k)
    """
    cfg = config or PotentialConfig()

    # Î£ d_u
    deviation_sum = sum(m.d for m in metrics)

    # Î£ w(e) for bad coupling edges
    edge_weight_sum = sum(
        e.weight_total for e in edges
        if e.is_boundary_crossing() or e.weight_components.coupling > 0.5
    )

    # Î¦ ê³„ì‚°
    phi = (
        cfg.lambda_1 * deviation_sum +
        cfg.lambda_2 * edge_weight_sum +
        cfg.lambda_3 * ops_penalty
    )

    return PotentialResult(
        phi=phi,
        deviation_sum=deviation_sum,
        edge_weight_sum=edge_weight_sum,
        ops_penalty=ops_penalty,
        components={
            "deviation": cfg.lambda_1 * deviation_sum,
            "edge": cfg.lambda_2 * edge_weight_sum,
            "ops": cfg.lambda_3 * ops_penalty,
        },
    )
```

### 5.2 Î”Î¦ ê³„ì‚° (delta.py)

```python
@dataclass
class DeltaPhiResult:
    """Î”Î¦ ê³„ì‚° ê²°ê³¼"""
    delta_phi: float
    phi_before: float
    phi_after: float
    improved: bool           # Î”Î¦ < 0


def calculate_delta_phi(
    phi_before: PotentialResult,
    phi_after: PotentialResult,
) -> DeltaPhiResult:
    """
    ì—ë„ˆì§€ ë³€í™”ëŸ‰ ê³„ì‚°

    Î”Î¦ = Î¦(after) - Î¦(before)

    Î”Î¦ < 0: ì—ë„ˆì§€ ê°ì†Œ (ì¢‹ìŒ)
    Î”Î¦ > 0: ì—ë„ˆì§€ ì¦ê°€ (ë‚˜ì¨)
    """
    delta = phi_after.phi - phi_before.phi

    return DeltaPhiResult(
        delta_phi=delta,
        phi_before=phi_before.phi,
        phi_after=phi_after.phi,
        improved=delta < 0,
    )
```

### 5.3 ìˆ˜ë ´ íŒì • (convergence.py)

```python
@dataclass
class ConvergenceResult:
    """ìˆ˜ë ´ íŒì • ê²°ê³¼"""
    converged: bool
    delta_phi: float
    epsilon: float
    iterations: int          # ì—°ì† ìˆ˜ë ´ íšŸìˆ˜
    message: str


DEFAULT_EPSILON = 0.01
MIN_CONVERGENCE_ITERATIONS = 3


def check_convergence(
    delta_phi: float,
    epsilon: float = DEFAULT_EPSILON,
    previous_iterations: int = 0,
) -> ConvergenceResult:
    """
    Îµ-ìˆ˜ë ´ íŒì •

    |Î”Î¦| < Îµ â†’ ìˆ˜ë ´
    """
    is_converging = abs(delta_phi) < epsilon

    iterations = previous_iterations + 1 if is_converging else 0
    converged = iterations >= MIN_CONVERGENCE_ITERATIONS

    if converged:
        message = f"Converged: |Î”Î¦|={abs(delta_phi):.4f} < Îµ={epsilon} for {iterations} iterations"
    elif is_converging:
        message = f"Converging: |Î”Î¦|={abs(delta_phi):.4f} < Îµ={epsilon} ({iterations}/{MIN_CONVERGENCE_ITERATIONS})"
    else:
        message = f"Not converged: |Î”Î¦|={abs(delta_phi):.4f} â‰¥ Îµ={epsilon}"

    return ConvergenceResult(
        converged=converged,
        delta_phi=delta_phi,
        epsilon=epsilon,
        iterations=iterations,
        message=message,
    )


def can_issue_adr(
    convergence: ConvergenceResult,
    flux_stable: bool,
    evidence_complete: bool,
    gate_passed: bool,
) -> tuple[bool, str]:
    """
    ADR ë°œê¸‰ ê°€ëŠ¥ ì—¬ë¶€ íŒì •

    ì¡°ê±´:
    - |Î”Î¦| < Îµ (ìˆ˜ë ´)
    - Flux_boundary ì•ˆì •
    - Evidence ì™„ë¹„
    - Gate(G) = false (ì—¬ì „íˆ ì‹¤íŒ¨)
    """
    if not convergence.converged:
        return False, f"Not converged: {convergence.message}"

    if not flux_stable:
        return False, "Boundary flux is unstable"

    if not evidence_complete:
        return False, "Evidence is incomplete"

    if gate_passed:
        return False, "Gate passed - no ADR needed"

    return True, "ADR can be issued: Essential Complexity confirmed"
```

---

## 6. Flux ëª¨ë“ˆ (flux)

### 6.1 ê²½ê³„ íë¦„ ê³„ì‚° (boundary.py)

```python
@dataclass
class FluxResult:
    """ê²½ê³„ íë¦„ ê³„ì‚° ê²°ê³¼"""
    flux: float
    boundary_edge_count: int
    avg_weight_per_edge: float


def calculate_boundary_flux(edges: list[Edge]) -> FluxResult:
    """
    ê²½ê³„ íë¦„ ê³„ì‚°

    Flux_boundary(k) = Î£ w(e)  where boundary(e) = 1
    """
    boundary_edges = [e for e in edges if e.is_boundary_crossing()]

    flux = sum(e.weight_total for e in boundary_edges)
    count = len(boundary_edges)
    avg = flux / count if count > 0 else 0.0

    return FluxResult(
        flux=flux,
        boundary_edge_count=count,
        avg_weight_per_edge=avg,
    )
```

### 6.2 ê²½ê³„ ì•…í™” íƒì§€ (degradation.py)

```python
@dataclass
class DegradationResult:
    """ê²½ê³„ ì•…í™” íƒì§€ ê²°ê³¼"""
    degraded: bool
    delta_flux: float
    avg_load_exceeded: bool
    message: str


BOUNDARY_LOAD_THRESHOLD = 2.0  # Î±


def detect_boundary_degradation(
    flux_before: FluxResult,
    flux_after: FluxResult,
) -> DegradationResult:
    """
    ê²½ê³„ ì•…í™” íƒì§€

    ê²½ê³  ì¡°ê±´:
    - Î”Flux > 0 (ê²½ê³„ ì•½í™”)
    - Flux / |E_boundary| > Î± (ê²½ê³„ë‹¹ í‰ê·  ë¶€í•˜ ì´ˆê³¼)
    """
    delta_flux = flux_after.flux - flux_before.flux
    avg_load_exceeded = flux_after.avg_weight_per_edge > BOUNDARY_LOAD_THRESHOLD

    degraded = delta_flux > 0 or avg_load_exceeded

    if degraded:
        reasons = []
        if delta_flux > 0:
            reasons.append(f"Î”Flux={delta_flux:.2f} > 0")
        if avg_load_exceeded:
            reasons.append(f"avg_load={flux_after.avg_weight_per_edge:.2f} > {BOUNDARY_LOAD_THRESHOLD}")
        message = f"ğŸ Bread weakening: {', '.join(reasons)}"
    else:
        message = "Boundary stable"

    return DegradationResult(
        degraded=degraded,
        delta_flux=delta_flux,
        avg_load_exceeded=avg_load_exceeded,
        message=message,
    )
```

---

## 7. View ëª¨ë“ˆ (view)

### 7.1 Hotspot Trajectory (hotspot.py)

```python
@dataclass
class HotspotCandidate:
    """ì¸ì§€ ë¶•ê´´ í›„ë³´"""
    entity_id: str
    consecutive_increases: int
    current_d: float
    trend: list[float]       # d values over time
    severity: Literal["low", "medium", "high", "critical"]


HOTSPOT_WINDOW = 5           # w: ì—°ì† ì¦ê°€ ìœˆë„ìš°
RAW_SUM_THRESHOLD = 20.0


def detect_hotspots(
    entity_id: str,
    d_history: list[float],  # d values over snapshots
    raw_sum: float,
) -> HotspotCandidate | None:
    """
    View A: Hotspot Trajectory

    íƒì§€ ì¡°ê±´:
    - âˆ€i âˆˆ [k-w, k]: d_u(i) > d_u(i-1) (w ì—°ì† ì¦ê°€)
    - rawSumRatio(k) > threshold
    """
    if len(d_history) < 2:
        return None

    # ì—°ì† ì¦ê°€ íšŸìˆ˜ ê³„ì‚°
    consecutive = 0
    for i in range(len(d_history) - 1, 0, -1):
        if d_history[i] > d_history[i-1]:
            consecutive += 1
        else:
            break

    is_hotspot = consecutive >= HOTSPOT_WINDOW or raw_sum > RAW_SUM_THRESHOLD

    if not is_hotspot:
        return None

    # ì‹¬ê°ë„ íŒì •
    if consecutive >= HOTSPOT_WINDOW and raw_sum > RAW_SUM_THRESHOLD:
        severity = "critical"
    elif consecutive >= HOTSPOT_WINDOW:
        severity = "high"
    elif raw_sum > RAW_SUM_THRESHOLD:
        severity = "medium"
    else:
        severity = "low"

    return HotspotCandidate(
        entity_id=entity_id,
        consecutive_increases=consecutive,
        current_d=d_history[-1],
        trend=d_history[-HOTSPOT_WINDOW:] if len(d_history) >= HOTSPOT_WINDOW else d_history,
        severity=severity,
    )
```

### 7.2 ROI Ranking (roi.py)

```python
@dataclass
class RefactorCandidate:
    """ë¦¬íŒ©í† ë§ í›„ë³´"""
    delta_id: str            # refactoring identifier
    description: str
    delta_phi: float         # expected Î”Î¦
    cost: float
    roi: float               # -Î”Î¦ / Cost
    affected_entities: list[str]


@dataclass
class CostFactors:
    """ë¹„ìš© ìš”ì†Œ"""
    files_changed: int
    public_api_changed: int
    schema_changed: int
    policy_touched: int
    test_delta: int


COST_WEIGHTS = {
    "files": 1.0,            # Î·â‚
    "api": 3.0,              # Î·â‚‚
    "schema": 5.0,           # Î·â‚ƒ
    "policy": 4.0,           # Î·â‚„
    "test": 2.0,             # Î·â‚…
}


def calculate_cost(factors: CostFactors) -> float:
    """
    Cost(Î”) = Î·â‚Â·#filesChanged + Î·â‚‚Â·#publicAPIChanged + ...
    """
    return (
        COST_WEIGHTS["files"] * factors.files_changed +
        COST_WEIGHTS["api"] * factors.public_api_changed +
        COST_WEIGHTS["schema"] * factors.schema_changed +
        COST_WEIGHTS["policy"] * factors.policy_touched +
        COST_WEIGHTS["test"] * factors.test_delta
    )


def calculate_roi(delta_phi: float, cost: float) -> float:
    """
    ROI(Î”) = -Î”Î¦ / Cost(Î”)
    """
    if cost <= 0:
        return 0.0
    return -delta_phi / cost


def rank_refactor_candidates(
    candidates: list[RefactorCandidate],
    top_k: int = 10,
) -> list[RefactorCandidate]:
    """
    View C: ROI ê¸°ì¤€ ì •ë ¬

    í›„ë³´ ì •ë ¬: ROI(Î”â‚) > ROI(Î”â‚‚) > ... > ROI(Î”â‚™)
    """
    return sorted(candidates, key=lambda c: c.roi, reverse=True)[:top_k]


def format_for_llm(candidates: list[RefactorCandidate]) -> str:
    """LLM ì œê³µìš© í¬ë§·"""
    lines = ["Top-K ROI í›„ë³´:"]
    for i, c in enumerate(candidates, 1):
        lines.append(f"{i}. {c.description}")
        lines.append(f"   ROI={c.roi:.2f}, Cost={c.cost:.1f}, -Î”Î¦={-c.delta_phi:.2f}")
    return "\n".join(lines)
```

---

## 8. ADR/Waiver ëª¨ë“ˆ (gate/adr)

### 8.1 ADR ìŠ¤í‚¤ë§ˆ (schema.py)

```python
from dataclasses import dataclass, field
from datetime import date, timedelta
from enum import Enum


class ADRStatus(Enum):
    DRAFT = "draft"
    APPROVED = "approved"
    DEPRECATED = "deprecated"
    SUPERSEDED = "superseded"


class ExpiryStatus(Enum):
    ACTIVE = "active"
    WARNING = "warning"
    EXPIRED = "expired"


@dataclass
class ApprovalInfo:
    approved_date: date
    grace_period: timedelta
    approver: str

    @property
    def expiry_date(self) -> date:
        return self.approved_date + self.grace_period


@dataclass
class ConvergenceProof:
    """ìˆ˜ë ´ ì¦ëª…"""
    snapshot_before: str
    snapshot_after: str
    delta_phi: float
    epsilon: float
    iterations: int
    evidence_complete: bool

    @property
    def is_valid(self) -> bool:
        return (
            abs(self.delta_phi) < self.epsilon and
            self.iterations >= MIN_CONVERGENCE_ITERATIONS and
            self.evidence_complete
        )


@dataclass
class TargetMetrics:
    """íƒ€ê²Ÿë³„ ë©”íŠ¸ë¦­"""
    x: list[float]           # [C, N, S, A, Î›]
    d: float
    hodge: str


@dataclass
class TargetFile:
    path: str
    signals: list[str] = field(default_factory=list)
    metrics: TargetMetrics | None = None


@dataclass
class Thresholds:
    nesting: int | None = None
    concepts: int | None = None


@dataclass
class ADRDocument:
    """ADR ë¬¸ì„œ (ìˆ˜ë ´ ì¦ëª… í¬í•¨)"""
    schema_version: str
    id: str
    title: str
    status: ADRStatus
    approval: ApprovalInfo
    convergence: ConvergenceProof     # NEW: ìˆ˜ë ´ ì¦ëª…
    targets: list[TargetFile]
    thresholds: Thresholds
    rationale: str
    references: list[str] = field(default_factory=list)

    def get_target(self, file_path: str) -> TargetFile | None:
        normalized = file_path.replace("\\", "/")
        for target in self.targets:
            if normalized.endswith(target.path) or target.path in normalized:
                return target
        return None

    def is_applicable(self, file_path: str) -> bool:
        return self.get_target(file_path) is not None
```

### 8.2 ê²€ì¦ê¸° (validator.py) - ìˆ˜ë ´ ê²€ì¦ ì¶”ê°€

```python
@dataclass
class ValidationError:
    field: str
    message: str
    severity: Literal["error", "warning"]


@dataclass
class ValidationResult:
    valid: bool
    errors: list[ValidationError]
    warnings: list[ValidationError]


class ADRValidator:
    """ADR ìœ íš¨ì„± ê²€ì¦ê¸° (ìˆ˜ë ´ ê²€ì¦ í¬í•¨)"""

    def validate(self, adr: ADRDocument) -> ValidationResult:
        errors: list[ValidationError] = []
        warnings: list[ValidationError] = []

        # ê¸°ì¡´ ê²€ì¦
        self._validate_required_fields(adr, errors)
        self._validate_status(adr, errors, warnings)
        self._validate_grace_period(adr, errors, warnings)
        self._validate_thresholds(adr, errors)
        self._validate_signals(adr, errors, warnings)

        # NEW: ìˆ˜ë ´ ì¦ëª… ê²€ì¦
        self._validate_convergence(adr, errors)

        return ValidationResult(
            valid=len(errors) == 0,
            errors=errors,
            warnings=warnings,
        )

    def _validate_convergence(self, adr: ADRDocument, errors: list[ValidationError]):
        """ìˆ˜ë ´ ì¦ëª… ê²€ì¦"""
        conv = adr.convergence

        # Îµ-ìˆ˜ë ´ ì¡°ê±´
        if abs(conv.delta_phi) >= conv.epsilon:
            errors.append(ValidationError(
                "convergence.delta_phi",
                f"|Î”Î¦|={abs(conv.delta_phi):.4f} â‰¥ Îµ={conv.epsilon} - not converged",
                "error"
            ))

        # ì—°ì† ìˆ˜ë ´ íšŸìˆ˜
        if conv.iterations < MIN_CONVERGENCE_ITERATIONS:
            errors.append(ValidationError(
                "convergence.iterations",
                f"iterations={conv.iterations} < {MIN_CONVERGENCE_ITERATIONS} required",
                "error"
            ))

        # Evidence ì™„ë¹„
        if not conv.evidence_complete:
            errors.append(ValidationError(
                "convergence.evidence_complete",
                "Evidence is incomplete",
                "error"
            ))

        # ìŠ¤ëƒ…ìƒ· ì¡´ì¬ í™•ì¸ (ì‹¤ì œ ê²€ì¦ì€ ì™¸ë¶€ì—ì„œ)
        if not conv.snapshot_before or not conv.snapshot_after:
            errors.append(ValidationError(
                "convergence.snapshots",
                "Snapshot references required",
                "error"
            ))
```

---

## 9. MCP í†µí•©

### 9.1 check_gate ìˆ˜ì •

```python
@mcp.tool()
def check_gate(
    source: str,
    file_path: str | None = None,
    gate_type: str = "mvp",
    project_root: str | None = None,
) -> dict:
    """Gate ê²€ì‚¬ + Waiver ì •ë³´ + ìˆ˜ë ´ ìƒíƒœ"""

    # ê¸°ì¡´ Gate ë¡œì§...

    # Waiver ì •ë³´
    waiver_info = None
    if result.cheese.waiver:
        w = result.cheese.waiver
        waiver_info = {
            "applied": result.cheese.waived,
            "adr_path": w.adr_path,
            "status": w.expiry.status.value if w.expiry else None,
            "expiry_date": str(w.expiry.expiry_date) if w.expiry else None,
            "remaining_days": w.expiry.remaining_days if w.expiry else None,
            "convergence": {
                "delta_phi": w.adr.convergence.delta_phi if w.adr else None,
                "epsilon": w.adr.convergence.epsilon if w.adr else None,
                "converged": w.adr.convergence.is_valid if w.adr else False,
            } if w.adr and w.adr.convergence else None,
            "adjustments": w.adjustments,
        }

    return {
        # ... ê¸°ì¡´ í•„ë“œ ...
        "waiver": waiver_info,
    }


@mcp.tool()
def check_adr_eligibility(
    source: str,
    file_path: str,
    snapshot_before: str,
    snapshot_after: str,
) -> dict:
    """ADR ë°œê¸‰ ìê²© í™•ì¸"""

    # ì¸¡ì •
    metrics_before = measure_metrics(source, snapshot_before)
    metrics_after = measure_metrics(source, snapshot_after)

    # ì—ë„ˆì§€ ê³„ì‚°
    phi_before = calculate_phi(metrics_before)
    phi_after = calculate_phi(metrics_after)
    delta = calculate_delta_phi(phi_before, phi_after)

    # ìˆ˜ë ´ íŒì •
    convergence = check_convergence(delta.delta_phi)

    # Flux ê³„ì‚°
    flux_before = calculate_boundary_flux(edges_before)
    flux_after = calculate_boundary_flux(edges_after)
    degradation = detect_boundary_degradation(flux_before, flux_after)

    # Evidence í™•ì¸
    evidence_complete = all(m.rule_hits for m in metrics_after)

    # Gate ê²°ê³¼
    gate_result = check_gate(source, file_path)

    # ADR ë°œê¸‰ ê°€ëŠ¥ ì—¬ë¶€
    can_issue, reason = can_issue_adr(
        convergence=convergence,
        flux_stable=not degradation.degraded,
        evidence_complete=evidence_complete,
        gate_passed=gate_result["passed"],
    )

    return {
        "eligible": can_issue,
        "reason": reason,
        "convergence": {
            "converged": convergence.converged,
            "delta_phi": convergence.delta_phi,
            "epsilon": convergence.epsilon,
            "iterations": convergence.iterations,
        },
        "flux": {
            "stable": not degradation.degraded,
            "delta_flux": degradation.delta_flux,
            "message": degradation.message,
        },
        "evidence": {
            "complete": evidence_complete,
        },
        "gate": {
            "passed": gate_result["passed"],
        },
    }
```

---

## 10. í…ŒìŠ¤íŠ¸ ê³„íš

### 10.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

| ëª¨ë“ˆ | í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ |
|------|---------------|
| measurement/vector.py | 5D ë²¡í„° ì¸¡ì •, rule_hits ìƒì„± |
| measurement/deviation.py | ì •ì¤€ í¸ì°¨ ê³„ì‚°, Î”d ê³„ì‚° |
| measurement/hodge.py | Hodge bucket ë¶„ë¥˜ |
| energy/potential.py | Î¦(k) ê³„ì‚° |
| energy/convergence.py | Îµ-ìˆ˜ë ´ íŒì • |
| flux/boundary.py | Flux ê³„ì‚° |
| flux/degradation.py | ê²½ê³„ ì•…í™” íƒì§€ |
| view/hotspot.py | Hotspot íƒì§€ |
| view/roi.py | ROI ê³„ì‚° ë° ì •ë ¬ |
| gate/adr/validator.py | ìˆ˜ë ´ ì¦ëª… ê²€ì¦ |

### 10.2 í†µí•© í…ŒìŠ¤íŠ¸

- ì „ì²´ íŒŒì´í”„ë¼ì¸: Source â†’ Metrics â†’ Î¦ â†’ Convergence â†’ ADR
- MCP check_gate + check_adr_eligibility ì—°ë™
- ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ ê¸°ë°˜ ìˆ˜ë ´ ì‹œë®¬ë ˆì´ì…˜

---

## 11. ì €ì¥ì†Œ ì•„í‚¤í…ì²˜

### 11.1 ì „ì²´ íŒŒì´í”„ë¼ì¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PIPELINE                                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Analyzer    â”‚â”€â”€â”€â–¶â”‚ Graph       â”‚â”€â”€â”€â–¶â”‚ Ingestor    â”‚â”€â”€â”€â–¶â”‚ Interpreter â”‚  â”‚
â”‚  â”‚ (ASTâ†’ë²¡í„°)  â”‚    â”‚ Builder     â”‚    â”‚ (DB ì ì¬)   â”‚    â”‚ (ì¿¼ë¦¬/íŒì •) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚        â”‚                  â”‚                  â”‚                  â”‚           â”‚
â”‚        â–¼                  â–¼                  â–¼                  â–¼           â”‚
â”‚   entities          edges            snapshots           Evidence          â”‚
â”‚   metrics         w_components       upsert             Packager           â”‚
â”‚   rule_hits                          versioning         (JSON íŒ¨í‚·)        â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.2 SQL ëª¨ë¸ (Postgres)

**ìš©ë„**: ì¶”ì /ê°ì‚¬/ë¦¬í¬íŠ¸, Î” ì¿¼ë¦¬, ì •í•©ì„±

```sql
-- 1) snapshots (ì»¤ë°‹ ë‹¨ìœ„)
CREATE TABLE snapshots (
  snapshot_id BIGSERIAL PRIMARY KEY,
  repo TEXT NOT NULL,
  commit_hash TEXT NOT NULL,
  release_id TEXT NOT NULL,           -- commit prefix(8)
  ts TIMESTAMPTZ NOT NULL,
  analyzer_version TEXT NOT NULL,
  profile_version TEXT NOT NULL,
  matrix_version TEXT NOT NULL,
  UNIQUE(repo, commit_hash)
);

-- 2) entities (ì •ì²´ì„±: ì‹œê°„ì— ë…ë¦½ì ì¸ ID)
CREATE TABLE entities (
  entity_id UUID PRIMARY KEY,
  repo TEXT NOT NULL,
  entity_type TEXT NOT NULL,          -- module|file|function|object
  language TEXT NOT NULL,
  path TEXT,
  symbol TEXT,
  fingerprint TEXT NOT NULL,          -- AST ê¸°ë°˜ stable fingerprint
  UNIQUE(repo, fingerprint)
);

-- 3) metrics (ì—”í‹°í‹° Ã— ìŠ¤ëƒ…ìƒ·)
CREATE TABLE metrics (
  snapshot_id BIGINT NOT NULL REFERENCES snapshots(snapshot_id),
  entity_id UUID NOT NULL REFERENCES entities(entity_id),
  architecture_role TEXT NOT NULL,
  module_confidence REAL NOT NULL,

  -- 5D ë²¡í„°
  c REAL NOT NULL,
  n REAL NOT NULL,
  s REAL NOT NULL,
  a REAL NOT NULL,
  lambda REAL NOT NULL,

  -- íŒŒìƒ ì ìˆ˜
  raw_sum REAL NOT NULL,
  raw_sum_threshold REAL NOT NULL,
  raw_sum_ratio REAL NOT NULL,
  tensor_score REAL NOT NULL,
  tensor_zone TEXT NOT NULL,          -- safe|review|violation
  canonical_deviation REAL NOT NULL,

  -- Hodge bucket
  h_alg INT NOT NULL,
  h_bal INT NOT NULL,
  h_arch INT NOT NULL,

  PRIMARY KEY(snapshot_id, entity_id)
);

-- 4) rule_hits (ê·¼ê±°: ì™œ ì ìˆ˜ê°€ ì´ë ‡ê²Œ ë‚˜ì™”ë‚˜)
CREATE TABLE rule_hits (
  snapshot_id BIGINT NOT NULL REFERENCES snapshots(snapshot_id),
  entity_id UUID NOT NULL REFERENCES entities(entity_id),
  rule_id TEXT NOT NULL,
  hit_count INT NOT NULL,
  locations JSONB,                    -- [{path, startLine, endLine, nodeType}, ...]
  PRIMARY KEY(snapshot_id, entity_id, rule_id)
);

-- 5) edges (ìŠ¤ëƒ…ìƒ·ë³„ ê·¸ë˜í”„)
CREATE TABLE edges (
  snapshot_id BIGINT NOT NULL REFERENCES snapshots(snapshot_id),
  src_entity_id UUID NOT NULL REFERENCES entities(entity_id),
  dst_entity_id UUID NOT NULL REFERENCES entities(entity_id),
  edge_type TEXT NOT NULL,            -- import|call|data|auth|deploy
  w_total REAL NOT NULL,
  w_components JSONB,                 -- {coupling, boundary, cognitive, failure}
  PRIMARY KEY(snapshot_id, src_entity_id, dst_entity_id, edge_type)
);
```

### 11.3 ì¸ë±ìŠ¤

```sql
-- Hotspot ì¡°íšŒ
CREATE INDEX idx_metrics_hotspot
  ON metrics(snapshot_id, canonical_deviation DESC);

-- rawSumRatio ì¡°íšŒ
CREATE INDEX idx_metrics_raw_ratio
  ON metrics(snapshot_id, raw_sum_ratio DESC);

-- Boundary edges ì¡°íšŒ
CREATE INDEX idx_edges_boundary
  ON edges(snapshot_id, edge_type, w_total DESC);

-- ê²½ë¡œ ê¸°ë°˜ ì¡°íšŒ
CREATE INDEX idx_entities_path
  ON entities(repo, path);

-- ê·œì¹™ë³„ íˆíŠ¸ ì¡°íšŒ
CREATE INDEX idx_rule_hits_rule
  ON rule_hits(snapshot_id, rule_id);
```

### 11.4 í•µì‹¬ ì¿¼ë¦¬

#### (A) Hotspot Top 20

```sql
SELECT e.entity_type, e.path, e.symbol,
       m.canonical_deviation, m.tensor_score, m.raw_sum_ratio
FROM metrics m
JOIN entities e ON e.entity_id = m.entity_id
WHERE m.snapshot_id = :sid
ORDER BY m.canonical_deviation DESC
LIMIT 20;
```

#### (B) Î”(ì¦ê°€ëŸ‰) ìœ„í—˜ ê¸‰ì¦ ì—”í‹°í‹°

```sql
WITH cur AS (
  SELECT entity_id, canonical_deviation AS d
  FROM metrics WHERE snapshot_id = :sid
),
prev AS (
  SELECT entity_id, canonical_deviation AS d
  FROM metrics WHERE snapshot_id = :sid_prev
)
SELECT e.path, e.symbol, (cur.d - prev.d) AS delta_d
FROM cur JOIN prev USING(entity_id)
JOIN entities e ON e.entity_id = cur.entity_id
ORDER BY delta_d DESC
LIMIT 50;
```

#### (C) Boundary Flux (ğŸ ì–‡ì•„ì§€ëŠ”ì§€)

```sql
SELECT SUM(w_total) AS boundary_flux
FROM edges
WHERE snapshot_id = :sid
  AND (w_components->>'boundary')::float > 0;
```

#### (D) ìˆ˜ë ´ ìƒíƒœ í™•ì¸ (ì—°ì† Î”Î¦)

```sql
WITH phi_history AS (
  SELECT snapshot_id,
         SUM(canonical_deviation) AS phi,
         LAG(SUM(canonical_deviation)) OVER (ORDER BY snapshot_id) AS phi_prev
  FROM metrics
  WHERE snapshot_id IN (:recent_sids)
  GROUP BY snapshot_id
)
SELECT snapshot_id,
       phi,
       (phi - phi_prev) AS delta_phi,
       ABS(phi - phi_prev) < :epsilon AS converging
FROM phi_history
ORDER BY snapshot_id;
```

### 11.5 NoSQL ëª¨ë¸ (MongoDB/Document)

**ìš©ë„**: Evidence packet ì•„ì¹´ì´ë¸Œ, ê°€ë³€ êµ¬ì¡°, ë¦´ë¦¬ìŠ¤ ìŠ¹ì¸ íŒ¨í‚·

```json
// snapshots/{repo}/{commit_hash}
{
  "repo": "semantic-complexity",
  "commit": "abcdef1234...",
  "releaseId": "abcdef12",
  "ts": "2025-12-31T05:00:00Z",
  "versions": {
    "analyzer": "0.0.13",
    "profiles": "2025-12-30",
    "matrix": "m1"
  },
  "entities": [
    {
      "entityId": "uuid-1234",
      "type": "function",
      "path": "src/py/semantic_complexity/analyzers/cheese.py",
      "symbol": "semantic_complexity.analyzers.cheese.analyze_cheese",
      "architectureRole": { "inferred": "lib/domain", "confidence": 0.95 },
      "vector": { "C": 12, "N": 7, "S": 3, "A": 0, "L": 5 },
      "scores": {
        "rawSum": 27,
        "rawSumRatio": 0.74,
        "tensor": 12.5,
        "deviation": 0.23
      },
      "hodge": { "alg": 19, "bal": 0, "arch": 8 },
      "ruleHits": [
        {
          "ruleId": "nesting/depth",
          "count": 7,
          "locations": [
            { "line": 120, "nodeType": "FunctionDef" },
            { "line": 145, "nodeType": "If" }
          ]
        },
        {
          "ruleId": "control/branch",
          "count": 12,
          "locations": [...]
        }
      ]
    }
  ],
  "edges": [
    {
      "src": "uuid-1234",
      "dst": "uuid-5678",
      "type": "import",
      "w": 2.1,
      "components": { "coupling": 0.8, "boundary": 0, "cognitive": 0.5, "failure": 0.8 }
    }
  ],
  "summary": {
    "totalEntities": 45,
    "totalEdges": 120,
    "phi": 15.23,
    "boundaryFlux": 8.5
  },
  "convergence": {
    "deltaPhi": 0.005,
    "epsilon": 0.01,
    "iterations": 3,
    "converged": true
  }
}
```

### 11.6 Graph DB (Neo4j) - ì„ íƒ

**ìš©ë„**: êµ¬ì¡°ì  í•´ì„, ê²½ë¡œ/ì»¤ë®¤ë‹ˆí‹°/ì¤‘ì‹¬ì„± ë¶„ì„

```cypher
// ë…¸ë“œ ìƒì„±
CREATE (e:Entity {
  entityId: 'uuid-1234',
  type: 'function',
  path: 'src/py/semantic_complexity/analyzers/cheese.py',
  symbol: 'analyze_cheese',
  deviation: 0.23
})

// ê´€ê³„ ìƒì„±
MATCH (src:Entity {entityId: 'uuid-1234'})
MATCH (dst:Entity {entityId: 'uuid-5678'})
CREATE (src)-[:CALLS {w: 2.1, boundary: 0.5}]->(dst)

// Boundary crossingì´ ë§ì€ í•¨ìˆ˜ ì£¼ë³€ 2-hop ì„œë¸Œê·¸ë˜í”„
MATCH path = (e:Entity)-[r*1..2]-(neighbor)
WHERE e.deviation > 0.5
  AND ANY(rel IN relationships(path) WHERE rel.boundary > 0)
RETURN path
LIMIT 100

// Hotspot ì˜í–¥ ë²”ìœ„ ë¶„ì„
MATCH (hotspot:Entity {entityId: :hotspot_id})
MATCH (hotspot)-[*1..3]-(affected)
RETURN DISTINCT affected.path, affected.symbol, affected.deviation
ORDER BY affected.deviation DESC
```

### 11.7 ê¶Œì¥ êµ¬ì„±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STORAGE ARCHITECTURE                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Postgres (ë©”ì¸)                                                      â”‚    â”‚
â”‚  â”‚ - entities, metrics, edges, snapshots, rule_hits                    â”‚    â”‚
â”‚  â”‚ - ì •í•©ì„± ë³´ì¥, Î” ì¿¼ë¦¬, ë¦¬í¬íŠ¸                                        â”‚    â”‚
â”‚  â”‚ - Hotspot/Flux/ROI ë¶„ì„                                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                               â”‚
â”‚                              â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Document Store (ë³´ì¡°)                                                â”‚    â”‚
â”‚  â”‚ - MongoDB / JSON files                                               â”‚    â”‚
â”‚  â”‚ - Evidence packet ì›ë³¸ ì•„ì¹´ì´ë¸Œ                                      â”‚    â”‚
â”‚  â”‚ - ê°ì‚¬/ìŠ¹ì¸ ì²¨ë¶€ìš©                                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                               â”‚
â”‚                              â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Neo4j (ì„ íƒ)                                                         â”‚    â”‚
â”‚  â”‚ - êµ¬ì¡°ì  í•´ì„ (ê²½ë¡œ, ì»¤ë®¤ë‹ˆí‹°, ì¤‘ì‹¬ì„±)                               â”‚    â”‚
â”‚  â”‚ - "ì™œ ì—¬ê¸°ê°€ hotspotì¸ê°€?" êµ¬ì¡°ì  ì„¤ëª…                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.8 Self-Validation ì „ëµ

```
semantic-complexity ìê¸° ê²€ì¦:
1. ìì‹ ì˜ ì†ŒìŠ¤ì½”ë“œë¥¼ ë¶„ì„
2. metrics/edges/rule_hits ìƒì„±
3. Î¦(k) ê³„ì‚°
4. ì´ì „ ìŠ¤ëƒ…ìƒ·ê³¼ ë¹„êµ â†’ Î”Î¦
5. ìˆ˜ë ´ ìƒíƒœ í™•ì¸
6. Gate í†µê³¼ ì—¬ë¶€ íŒì •

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Self-Validation Pipeline                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  semantic-complexity/    â”€â”€â–¶  Analyzer  â”€â”€â–¶  DB ì ì¬           â”‚
â”‚  (source)                                       â”‚               â”‚
â”‚                                                 â–¼               â”‚
â”‚                                          Î¦(k) ê³„ì‚°             â”‚
â”‚                                                 â”‚               â”‚
â”‚                                                 â–¼               â”‚
â”‚  semantic-complexity/    â—€â”€â”€  Gate íŒì •  â—€â”€â”€  Î”Î¦ ë¹„êµ         â”‚
â”‚  (validated)                                                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 12. Python ì €ì¥ì†Œ êµ¬í˜„ (Lightweight)

### 12.1 SQLite ê¸°ë°˜ (ë¡œì»¬ ê°œë°œìš©)

```python
# graph/store.py
import sqlite3
import json
from pathlib import Path
from dataclasses import asdict


class SQLiteStore:
    """SQLite ê¸°ë°˜ ë¡œì»¬ ì €ì¥ì†Œ"""

    SCHEMA = """
    CREATE TABLE IF NOT EXISTS snapshots (
        snapshot_id INTEGER PRIMARY KEY AUTOINCREMENT,
        repo TEXT NOT NULL,
        commit_hash TEXT NOT NULL,
        release_id TEXT NOT NULL,
        ts TEXT NOT NULL,
        analyzer_version TEXT NOT NULL,
        UNIQUE(repo, commit_hash)
    );

    CREATE TABLE IF NOT EXISTS entities (
        entity_id TEXT PRIMARY KEY,
        repo TEXT NOT NULL,
        entity_type TEXT NOT NULL,
        language TEXT NOT NULL,
        path TEXT,
        symbol TEXT,
        fingerprint TEXT NOT NULL,
        UNIQUE(repo, fingerprint)
    );

    CREATE TABLE IF NOT EXISTS metrics (
        snapshot_id INTEGER NOT NULL,
        entity_id TEXT NOT NULL,
        architecture_role TEXT NOT NULL,
        c REAL, n REAL, s REAL, a REAL, lambda REAL,
        raw_sum REAL,
        canonical_deviation REAL,
        h_alg INTEGER, h_bal INTEGER, h_arch INTEGER,
        PRIMARY KEY(snapshot_id, entity_id)
    );

    CREATE TABLE IF NOT EXISTS rule_hits (
        snapshot_id INTEGER NOT NULL,
        entity_id TEXT NOT NULL,
        rule_id TEXT NOT NULL,
        hit_count INTEGER NOT NULL,
        locations TEXT,
        PRIMARY KEY(snapshot_id, entity_id, rule_id)
    );

    CREATE TABLE IF NOT EXISTS edges (
        snapshot_id INTEGER NOT NULL,
        src_entity_id TEXT NOT NULL,
        dst_entity_id TEXT NOT NULL,
        edge_type TEXT NOT NULL,
        w_total REAL NOT NULL,
        w_components TEXT,
        PRIMARY KEY(snapshot_id, src_entity_id, dst_entity_id, edge_type)
    );

    CREATE INDEX IF NOT EXISTS idx_metrics_deviation
        ON metrics(snapshot_id, canonical_deviation DESC);
    """

    def __init__(self, db_path: Path | str = ":memory:"):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        self._init_schema()

    def _init_schema(self):
        self.conn.executescript(self.SCHEMA)
        self.conn.commit()

    def insert_snapshot(self, snapshot: Snapshot) -> int:
        cursor = self.conn.execute("""
            INSERT INTO snapshots (repo, commit_hash, release_id, ts, analyzer_version)
            VALUES (?, ?, ?, ?, ?)
            ON CONFLICT(repo, commit_hash) DO UPDATE SET ts = excluded.ts
            RETURNING snapshot_id
        """, (snapshot.repo, snapshot.commit, snapshot.commit[:8],
              snapshot.timestamp.isoformat(), "0.0.13"))
        self.conn.commit()
        return cursor.fetchone()[0]

    def insert_metrics(self, snapshot_id: int, metrics: Metrics):
        self.conn.execute("""
            INSERT OR REPLACE INTO metrics
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            snapshot_id, metrics.entity_id, metrics.architecture_role,
            metrics.x.C, metrics.x.N, metrics.x.S, metrics.x.A, metrics.x.L,
            metrics.raw_sum, metrics.d,
            int(metrics.x.C + metrics.x.N),  # h_alg
            int(metrics.x.A),                 # h_bal
            int(metrics.x.S + metrics.x.L),   # h_arch
        ))
        self.conn.commit()

    def get_hotspots(self, snapshot_id: int, limit: int = 20) -> list[dict]:
        cursor = self.conn.execute("""
            SELECT e.entity_type, e.path, e.symbol,
                   m.canonical_deviation, m.raw_sum
            FROM metrics m
            JOIN entities e ON e.entity_id = m.entity_id
            WHERE m.snapshot_id = ?
            ORDER BY m.canonical_deviation DESC
            LIMIT ?
        """, (snapshot_id, limit))
        return [dict(row) for row in cursor.fetchall()]

    def get_delta_deviations(self, sid_cur: int, sid_prev: int) -> list[dict]:
        cursor = self.conn.execute("""
            WITH cur AS (
                SELECT entity_id, canonical_deviation AS d FROM metrics WHERE snapshot_id = ?
            ),
            prev AS (
                SELECT entity_id, canonical_deviation AS d FROM metrics WHERE snapshot_id = ?
            )
            SELECT e.path, e.symbol, (cur.d - prev.d) AS delta_d
            FROM cur JOIN prev USING(entity_id)
            JOIN entities e ON e.entity_id = cur.entity_id
            ORDER BY delta_d DESC
            LIMIT 50
        """, (sid_cur, sid_prev))
        return [dict(row) for row in cursor.fetchall()]

    def get_boundary_flux(self, snapshot_id: int) -> float:
        cursor = self.conn.execute("""
            SELECT SUM(w_total) AS flux
            FROM edges
            WHERE snapshot_id = ?
              AND json_extract(w_components, '$.boundary') > 0
        """, (snapshot_id,))
        row = cursor.fetchone()
        return row["flux"] or 0.0
```

### 12.2 JSON ê¸°ë°˜ Evidence Packager

```python
# evidence/packager.py
import json
from datetime import datetime
from pathlib import Path
from dataclasses import asdict


class EvidencePackager:
    """Evidence packet JSON ìƒì„±ê¸°"""

    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def package(
        self,
        snapshot: Snapshot,
        entities: list[Entity],
        metrics: list[Metrics],
        rule_hits: list[RuleHit],
        edges: list[Edge],
        convergence: ConvergenceResult | None = None,
    ) -> Path:
        """Evidence packet ìƒì„±"""
        packet = {
            "repo": snapshot.repo,
            "commit": snapshot.commit,
            "releaseId": snapshot.commit[:8],
            "ts": snapshot.timestamp.isoformat(),
            "versions": {
                "analyzer": "0.0.13",
                "profiles": datetime.now().strftime("%Y-%m-%d"),
            },
            "entities": self._build_entities(entities, metrics, rule_hits),
            "edges": self._build_edges(edges),
            "summary": self._build_summary(metrics, edges),
        }

        if convergence:
            packet["convergence"] = {
                "deltaPhi": convergence.delta_phi,
                "epsilon": convergence.epsilon,
                "iterations": convergence.iterations,
                "converged": convergence.converged,
            }

        # ì €ì¥
        filename = f"{snapshot.repo}_{snapshot.commit[:8]}.json"
        output_path = self.output_dir / filename
        output_path.write_text(json.dumps(packet, indent=2, ensure_ascii=False))

        return output_path

    def _build_entities(
        self,
        entities: list[Entity],
        metrics: list[Metrics],
        rule_hits: list[RuleHit],
    ) -> list[dict]:
        metrics_map = {m.entity_id: m for m in metrics}
        hits_map: dict[str, list[RuleHit]] = {}
        for h in rule_hits:
            hits_map.setdefault(h.entity_id, []).append(h)

        result = []
        for e in entities:
            m = metrics_map.get(e.entity_id)
            hits = hits_map.get(e.entity_id, [])

            entity_data = {
                "entityId": e.entity_id,
                "type": e.type.value,
                "path": e.path,
                "symbol": e.symbol,
            }

            if m:
                entity_data["architectureRole"] = {
                    "inferred": m.architecture_role,
                    "confidence": m.confidence,
                }
                entity_data["vector"] = {
                    "C": m.x.C, "N": m.x.N, "S": m.x.S, "A": m.x.A, "L": m.x.L
                }
                entity_data["scores"] = {
                    "rawSum": m.raw_sum,
                    "deviation": m.d,
                }
                entity_data["hodge"] = {
                    "alg": int(m.x.C + m.x.N),
                    "bal": int(m.x.A),
                    "arch": int(m.x.S + m.x.L),
                }

            if hits:
                entity_data["ruleHits"] = [
                    {
                        "ruleId": h.rule_id,
                        "count": h.count,
                        "locations": [asdict(loc) for loc in h.locations],
                    }
                    for h in hits
                ]

            result.append(entity_data)

        return result

    def _build_edges(self, edges: list[Edge]) -> list[dict]:
        return [
            {
                "src": e.src_entity,
                "dst": e.dst_entity,
                "type": e.edge_type.value,
                "w": e.weight_total,
                "components": asdict(e.weight_components),
            }
            for e in edges
        ]

    def _build_summary(self, metrics: list[Metrics], edges: list[Edge]) -> dict:
        phi = sum(m.d for m in metrics)
        boundary_flux = sum(
            e.weight_total for e in edges
            if e.is_boundary_crossing()
        )
        return {
            "totalEntities": len(metrics),
            "totalEdges": len(edges),
            "phi": phi,
            "boundaryFlux": boundary_flux,
        }
```

---

## 13. ì°¸ì¡°

- [SRS-WAIVER.ko.md](SRS-WAIVER.ko.md) - ìš”êµ¬ì‚¬í•­ ëª…ì„¸
- [THEORY.ko.md](THEORY.ko.md) - Ham Sandwich ì´ë¡ ì  ê¸°ë°˜
- Lyapunov Stability Theory - ì—ë„ˆì§€ í•¨ìˆ˜ ìˆ˜ë ´ ë¶„ì„
- PostgreSQL JSON Functions - w_components ì¿¼ë¦¬
- SQLite JSON1 Extension - ë¡œì»¬ ê°œë°œìš©
